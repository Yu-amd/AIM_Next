apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: aim-inference-with-guardrails
  namespace: aim-guardrails
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  transformer:
    containers:
      - name: guardrail-transformer
        image: aim-guardrails:latest
        command: ["python3", "-m", "guardrails.kserve.server"]
        env:
          - name: GUARDRAIL_CONFIG_YAML
            value: "/etc/guardrails/config.yaml"
          - name: ENABLE_METRICS
            value: "true"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        ports:
          - containerPort: 8080
            name: http
          - containerPort: 9090
            name: metrics
        volumeMounts:
          - name: guardrail-config
            mountPath: /etc/guardrails
            readOnly: true
    volumes:
      - name: guardrail-config
        configMap:
          name: guardrail-policy-config
  predictor:
    # Your AIM inference service
    containers:
      - name: aim-inference
        image: aim-inference:latest  # Replace with your AIM image
        resources:
          requests:
            amd.com/gpu: "1"
            memory: "32Gi"
            cpu: "4000m"
          limits:
            amd.com/gpu: "1"
            memory: "64Gi"
            cpu: "8000m"
        ports:
          - containerPort: 8080
            name: http

---
apiVersion: v1
kind: Service
metadata:
  name: aim-inference-with-guardrails
  namespace: aim-guardrails
spec:
  selector:
    app: aim-inference-with-guardrails
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
      name: http

