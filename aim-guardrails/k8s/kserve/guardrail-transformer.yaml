apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: aim-guardrail-transformer
  namespace: aim-guardrails
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  transformer:
    containers:
      - name: kserve-container
        image: aim-guardrails:latest
        command: ["python3", "-m", "guardrails.kserve.server"]
        env:
          - name: GUARDRAIL_SERVICE_URL
            value: "http://aim-guardrail-service:8080"
          - name: ENABLE_METRICS
            value: "true"
          - name: PORT
            value: "8080"
        resources:
          requests:
            memory: "512Mi"
            cpu: "200m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        ports:
          - containerPort: 8080
            name: http
          - containerPort: 9090
            name: metrics
  predictor:
    # AIM inference service reference
    # This would be your actual AIM InferenceService
    containers:
      - name: aim-inference
        image: aim-inference:latest
        resources:
          requests:
            amd.com/gpu: "1"
            memory: "32Gi"
            cpu: "4000m"
          limits:
            amd.com/gpu: "1"
            memory: "64Gi"
            cpu: "8000m"

---
apiVersion: v1
kind: Service
metadata:
  name: aim-guardrail-transformer
  namespace: aim-guardrails
spec:
  selector:
    app: aim-guardrail-transformer
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
      name: http
    - protocol: TCP
      port: 9090
      targetPort: 9090
      name: metrics

